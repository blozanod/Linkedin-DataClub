# -*- coding: utf-8 -*-
"""timejobs scraper export

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lOuHNdfzN9Y4kMeh0ou2Glov57DULVGa
"""

#FINISHED SCRAPER - THIS ONE WORKS!
'''
TO-DO:
- align w kaggle cols DONE
'''

from bs4 import BeautifulSoup
import requests
import pandas as pd

#-----------------scraper!---------------------------------------
def getting_data(keywords, jobs_shown=10):
    """
    kaggle cols matched:
    - job_id - used job URL
    - company_name
    - title
    - description
    - location

    kaggle cols i couldn't find (default to NONE):
    - max_salary
    - pay_period
    - company_id
    - views
    - med_salary
    """
    jobs_list = []
    page = 1

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    }

    '''
    other headers:
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.9',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1'
    '''

    print(f"looking for jobs with these keywords: {keywords}")


    while True:
        try:


            url = f'https://www.timesjobs.com/candidate/job-search.html?from=submit&actualTxtKeywords={keywords}&searchBy=0&rdoOperator=OR&searchType=personalizedSearch&luceneResultSize=25&postWeek=60&txtKeywords={keywords}&pDate=I&sequence={page}&startPage=1'
                  #https://www.timesjobs.com/candidate/job-search.html?from=submit&actualTxtKeywords=data%20analyst&searchBy=0&rdoOperator=OR&searchType=personalizedSearch&luceneResultSize=25&postWeek=60&txtKeywords=data%20analyst&pDate=I&sequence=2&startPage=1

            response = requests.get(url, headers=headers)
            soup = BeautifulSoup(response.text, 'lxml')


            #stuff to check for errors------------------------
            print(f"  Status: {response.status_code}")

            if response.status_code != 200:
                print(f"scraper doesn't work rn! status code: {response.status_code}")
                break
            #-------------------------------------------------

            # Find all job listings
            jobs = soup.find_all('li', class_='clearfix job-bx wht-shd-bx')

            if not jobs:
                print("No jobs found")
                break

            for job in jobs:

                try:
                    # job_id ---------------------------------
                    '''
                    <li class="job-bx">
                        <h2>
                            <a href="https://www.timesjobs.com/job-detail/579213">
                                Software Engineer
                            </a>
                        </h2>
                        <h3>Google</h3>
                        <p>Job description...</p>
                    </li>
                    '''
                    h2_tag = job.find('h2')
                    if h2_tag:
                        a_tag = h2_tag.find('a')
                        if a_tag and a_tag.has_attr('href'):
                            job_id = a_tag['href']
                        else:
                            job_id = None
                    else:
                        job_id = None

                    # title (job title)------------------------------
                    '''
                    <h2>
                        <a href="https://www.timesjobs.com/job-detail/579213">Software Engineer</a>
                    </h2>
                    '''
                    title = None
                    if h2_tag:
                        a_tag = h2_tag.find('a')
                        if a_tag:
                            title = a_tag.get_text(strip=True)
                            # Clean up extra whitespace

                    # company_name------------------------------------
                    company_name = None
                    company_tag = job.find('h3', class_='joblist-comp-name')
                    if company_tag:
                        company_name = company_tag.get_text(strip=True)

                    # location----------------------------------------
                    location = None
                    location_ul = job.find('ul', class_='top-jd-dtl')
                    if location_ul:
                        location_li = location_ul.find('li', class_='location-tru')
                        if location_li:
                            location = location_li.get_text(strip=True)

                    # description - Job description-----------------------
                    description = None
                    desc_tag = job.find('li', class_='job-description__')
                    if desc_tag:
                        description = desc_tag.get_text(strip=True)

                    if not description:
                        desc_tag = job.find('ul', class_='list-job-dtl')
                        if desc_tag:
                            description = desc_tag.get_text(strip=True)

                    # stuff i couldn't find
                    max_salary = None      # Not on listing pages
                    pay_period = None      # Not available
                    company_id = None      # Not available
                    views = None           # Not available
                    med_salary = None      # Not available


                    one_job_data = {
                        'job_id': job_id,
                        'company_name': company_name,
                        'title': title,
                        'description': description,
                        'max_salary': max_salary,
                        'pay_period': pay_period,
                        'location': location,
                        'company_id': company_id,
                        'views': views,
                        'med_salary': med_salary
                    }

                    jobs_list.append(one_job_data)
                    page += 1 #this is wrong but idk where to put it

                except Exception as e:
                    print(f"error: {e}")
                    continue

            # cap off w jobs_shown
            if len(jobs_list) >= jobs_shown:
                break

        except Exception as e:
            print(f"error: {e}")
            break


    df = pd.DataFrame(jobs_list)
    print(f"total jobs: {len(df)}")
    return df

#convert to sqlalchemy

from sqlalchemy import create_engine
keywords = 'data analyst'
jobs_shown = 20

df = getting_data(keywords, jobs_shown=jobs_shown)
engine = create_engine('sqlite:///jobs.db')
df.to_sql('timesjobs', engine, if_exists='replace', index=False)
print(df)